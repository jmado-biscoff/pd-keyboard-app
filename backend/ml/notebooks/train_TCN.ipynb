{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e433c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28889015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "SEQ_LEN     = 32\n",
    "FEATS       = [\"Hold_Time\", \"DD\", \"UD\"]   # extend later if you add engineered features\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS      = 50\n",
    "PATIENCE    = 5\n",
    "LR          = 1e-3\n",
    "DEVICE      = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13fa0f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using raw CSVs from repo: c:\\Users\\geloq\\OneDrive\\Desktop\\pd-keyboard-app\\backend\\ml\\dataset\\raw  (found 6)\n",
      "CWD: c:\\Users\\geloq\\OneDrive\\Desktop\\pd-keyboard-app\\backend\\ml\\notebooks\n",
      "Example file: c:\\Users\\geloq\\OneDrive\\Desktop\\pd-keyboard-app\\backend\\ml\\dataset\\raw\\100_.tie5Roanl_keystroke_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Find project root & dataset paths robustly\n",
    "# -----------------------\n",
    "def find_project_root() -> str:\n",
    "    \"\"\"\n",
    "    Try current dir and up to 4 parents to locate a folder that contains 'backend/ml/dataset/raw'.\n",
    "    If not found, return current working directory.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    candidates = [cwd]\n",
    "    # try parents\n",
    "    cur = cwd\n",
    "    for _ in range(4):\n",
    "        cur = os.path.dirname(cur)\n",
    "        if cur and cur not in candidates:\n",
    "            candidates.append(cur)\n",
    "    for base in candidates:\n",
    "        raw_dir = os.path.join(base, \"backend\", \"ml\", \"dataset\", \"raw\")\n",
    "        if os.path.isdir(raw_dir):\n",
    "            return base\n",
    "    return cwd\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "RAW_DIR  = os.path.join(PROJECT_ROOT, \"backend\", \"ml\", \"dataset\", \"raw\")\n",
    "PROC_DIR = os.path.join(PROJECT_ROOT, \"backend\", \"ml\", \"dataset\", \"processed\")\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, \"backend\", \"ml\", \"models\")\n",
    "EVAL_DIR  = os.path.join(PROJECT_ROOT, \"backend\", \"ml\", \"evaluation\")\n",
    "\n",
    "os.makedirs(PROC_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "\n",
    "def find_raw_csvs() -> List[str]:\n",
    "    # Prefer repo raw folder\n",
    "    repo_paths = sorted(glob.glob(os.path.join(RAW_DIR, \"*.csv\")))\n",
    "    if repo_paths:\n",
    "        print(f\"Using raw CSVs from repo: {RAW_DIR}  (found {len(repo_paths)})\")\n",
    "        return repo_paths\n",
    "    # Fallback to /mnt/data for quick tests\n",
    "    mnt_paths = sorted(glob.glob(\"/mnt/data/*.csv\"))\n",
    "    if mnt_paths:\n",
    "        print(f\"Using raw CSVs from /mnt/data  (found {len(mnt_paths)})\")\n",
    "        return mnt_paths\n",
    "    return []\n",
    "\n",
    "raw_files = find_raw_csvs()\n",
    "if not raw_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No CSV files found.\\n\"\n",
    "        f\"Looked in:\\n - {RAW_DIR}/*.csv\\n - /mnt/data/*.csv\\n\"\n",
    "        f\"Tip: put your raw files in {RAW_DIR}\"\n",
    "    )\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Example file:\", raw_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6811547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Robust CSV loader\n",
    "# -----------------------\n",
    "def sniff_sep(sample: str) -> str:\n",
    "    # prefer ; if it appears more than commas in header line\n",
    "    first = sample.splitlines()[0]\n",
    "    if first.count(\";\") > first.count(\",\"): \n",
    "        return \";\"\n",
    "    return \",\"\n",
    "\n",
    "def load_csv_robust(path: str) -> pd.DataFrame:\n",
    "    # Try several strategies to avoid ParserError (bad rows, odd quoting, different sep, encodings)\n",
    "    try:\n",
    "        return pd.read_csv(path)  # fastest path\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Read a small header to guess delimiter\n",
    "    with open(path, \"rb\") as f:\n",
    "        head = f.read(8192)\n",
    "    try:\n",
    "        sample = head.decode(\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        sample = head.decode(\"latin-1\", errors=\"ignore\")\n",
    "    sep = sniff_sep(sample)\n",
    "\n",
    "    # Try python engine with autodetected sep and robust options\n",
    "    for enc in (\"utf-8\", \"latin-1\"):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                path,\n",
    "                sep=sep,\n",
    "                engine=\"python\",\n",
    "                encoding=enc,\n",
    "                on_bad_lines=\"skip\",     # skip malformed rows\n",
    "                quoting=0,               # QUOTE_MINIMAL\n",
    "                skip_blank_lines=True\n",
    "            )\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            return df\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Final fallback: manually filter rows with wrong field counts\n",
    "    rows = []\n",
    "    expected_fields: Optional[int] = None\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            parts = [p.strip() for p in line.rstrip(\"\\n\").split(sep)]\n",
    "            if expected_fields is None:\n",
    "                expected_fields = len(parts)\n",
    "                rows.append(parts)\n",
    "            else:\n",
    "                if len(parts) == expected_fields:\n",
    "                    rows.append(parts)\n",
    "                else:\n",
    "                    # skip malformed line\n",
    "                    continue\n",
    "    df = pd.DataFrame(rows[1:], columns=rows[0]) if rows else pd.DataFrame()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ee809dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Load & normalize all raw CSVs\n",
    "# -----------------------\n",
    "dfs = []\n",
    "for fp in raw_files:\n",
    "    df = load_csv_robust(fp)\n",
    "\n",
    "    # Required columns; some datasets use slightly different casingâ€”normalize column names\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    rename_map = {\n",
    "        \"user_id\":\"User_ID\", \"session_id\":\"Session_ID\",\n",
    "        \"press_time\":\"Press_Time\", \"release_time\":\"Release_Time\",\n",
    "        \"hold_time\":\"Hold_Time\"\n",
    "    }\n",
    "    for k,v in rename_map.items():\n",
    "        if k in df.columns and v not in df.columns: df.rename(columns={k:v}, inplace=True)\n",
    "\n",
    "    required = [\"User_ID\", \"Session_ID\", \"Press_Time\", \"Release_Time\"]\n",
    "    miss = [c for c in required if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"{os.path.basename(fp)} missing required columns: {miss}\")\n",
    "\n",
    "    # Derive Hold_Time/flight times if absent\n",
    "    if \"Hold_Time\" not in df.columns:\n",
    "        df[\"Hold_Time\"] = pd.to_numeric(df[\"Release_Time\"] - df[\"Press_Time\"], errors=\"coerce\")\n",
    "\n",
    "    if (\"DD\" not in df.columns) or (\"UD\" not in df.columns):\n",
    "        df = df.sort_values([\"User_ID\",\"Session_ID\",\"Press_Time\"]).reset_index(drop=True)\n",
    "        def flights(g):\n",
    "            g = g.sort_values(\"Press_Time\").copy()\n",
    "            g[\"DD\"] = g[\"Press_Time\"].diff().fillna(0.0)\n",
    "            g[\"UD\"] = g[\"Press_Time\"].values - g[\"Release_Time\"].shift(1).fillna(g[\"Press_Time\"]).values\n",
    "            return g\n",
    "        df = df.groupby([\"User_ID\",\"Session_ID\"], group_keys=False).apply(flights)\n",
    "\n",
    "    keep = [\"User_ID\",\"Session_ID\",\"Press_Time\",\"Release_Time\",\"Hold_Time\",\"DD\",\"UD\"]\n",
    "    for extra in [\"Key_Pressed\",\"Key_Pressed_Previous\",\"Characters_Count\"]:\n",
    "        if extra in df.columns: keep.append(extra)\n",
    "\n",
    "    # numeric cleanup\n",
    "    for c in [\"Hold_Time\",\"DD\",\"UD\",\"Press_Time\",\"Release_Time\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"Hold_Time\",\"DD\",\"UD\",\"Press_Time\",\"Release_Time\"]).reset_index(drop=True)\n",
    "\n",
    "    dfs.append(df[keep].copy())\n",
    "\n",
    "raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# tame long tails\n",
    "for c in [\"Hold_Time\",\"DD\",\"UD\"]:\n",
    "    lo, hi = raw[c].quantile(0.001), raw[c].quantile(0.999)\n",
    "    raw[c] = raw[c].clip(lo, hi).fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "288c7ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built windows: (7965, 32, 3) (7965, 32)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Weak labels via per-session IQR (1 = timing anomaly)\n",
    "# -----------------------\n",
    "def iqr_mask(s: pd.Series):\n",
    "    q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "    iqr = max(q3 - q1, 1e-6)\n",
    "    lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    return (s < lower) | (s > upper)\n",
    "\n",
    "labels = np.zeros(len(raw), dtype=np.int64)\n",
    "for (_, _), g in raw.groupby([\"User_ID\",\"Session_ID\"]):\n",
    "    idx = g.index\n",
    "    mask = iqr_mask(g[\"Hold_Time\"]) | iqr_mask(g[\"DD\"]) | iqr_mask(g[\"UD\"])\n",
    "    labels[idx] = mask.astype(np.int64)\n",
    "raw[\"label\"] = labels\n",
    "\n",
    "# -----------------------\n",
    "# Build windows (stride=1)\n",
    "# -----------------------\n",
    "def build_windows(df: pd.DataFrame, seq_len=SEQ_LEN, feat_cols=FEATS):\n",
    "    X_list, y_list = [], []\n",
    "    for (_uid, _sid), g in df.groupby([\"User_ID\",\"Session_ID\"]):\n",
    "        g = g.sort_values([\"Press_Time\",\"Release_Time\"]).reset_index(drop=True)\n",
    "        feats = g[feat_cols].values.astype(np.float32)\n",
    "        labs  = g[\"label\"].values.astype(np.int64)\n",
    "        T = len(g)\n",
    "        if T < seq_len: continue\n",
    "        for i in range(T - seq_len + 1):\n",
    "            X_list.append(feats[i:i+seq_len])\n",
    "            y_list.append(labs[i:i+seq_len])\n",
    "    if not X_list:\n",
    "        raise ValueError(\"No sequences created. Consider reducing SEQ_LEN or check raw files.\")\n",
    "    X = np.stack(X_list, axis=0)\n",
    "    y = np.stack(y_list, axis=0)\n",
    "    return X, y\n",
    "\n",
    "X, y = build_windows(raw, SEQ_LEN, FEATS)\n",
    "print(\"Built windows:\", X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a5a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed arrays saved to: c:\\Users\\geloq\\OneDrive\\Desktop\\pd-keyboard-app\\backend\\ml\\dataset\\processed\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Split â†’ Standardize â†’ Save processed\n",
    "# -----------------------\n",
    "N = len(X)\n",
    "idx = np.arange(N)\n",
    "idx_train, idx_tmp = train_test_split(idx, test_size=0.30, random_state=42, shuffle=True)\n",
    "idx_val, idx_test = train_test_split(idx_tmp, test_size=0.50, random_state=42, shuffle=True)\n",
    "\n",
    "X_train, y_train = X[idx_train], y[idx_train]\n",
    "X_val,   y_val   = X[idx_val],   y[idx_val]\n",
    "X_test,  y_test  = X[idx_test],  y[idx_test]\n",
    "\n",
    "mean = X_train.reshape(-1, X_train.shape[-1]).mean(axis=0)\n",
    "std  = X_train.reshape(-1, X_train.shape[-1]).std(axis=0) + 1e-6\n",
    "def standardize(a): return (a - mean) / std\n",
    "\n",
    "X_train = standardize(X_train).astype(np.float32)\n",
    "X_val   = standardize(X_val).astype(np.float32)\n",
    "X_test  = standardize(X_test).astype(np.float32)\n",
    "\n",
    "np.save(os.path.join(PROC_DIR, \"X_train.npy\"), X_train)\n",
    "np.save(os.path.join(PROC_DIR, \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(PROC_DIR, \"X_val.npy\"),   X_val)\n",
    "np.save(os.path.join(PROC_DIR, \"y_val.npy\"),   y_val)\n",
    "np.save(os.path.join(PROC_DIR, \"X_test.npy\"),  X_test)\n",
    "np.save(os.path.join(PROC_DIR, \"y_test.npy\"),  y_test)\n",
    "\n",
    "with open(os.path.join(PROC_DIR, \"feature_config.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"features\": FEATS,\n",
    "        \"scaler_mean\": mean.tolist(),\n",
    "        \"scaler_scale\": std.tolist()\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Processed arrays saved to: {PROC_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dba0caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Define TCN\n",
    "# -----------------------\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp): super().__init__(); self.chomp = chomp\n",
    "    def forward(self, x): return x[:, :, :-self.chomp] if self.chomp > 0 else x\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, stride, dilation, padding):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, k, stride=stride, padding=padding, dilation=dilation),\n",
    "            Chomp1d(padding), nn.ReLU(), nn.BatchNorm1d(out_ch),\n",
    "            nn.Conv1d(out_ch, out_ch, k, stride=stride, padding=padding, dilation=dilation),\n",
    "            Chomp1d(padding), nn.ReLU(), nn.BatchNorm1d(out_ch),\n",
    "        )\n",
    "        self.down = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.down is None else self.down(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, channels=(32,32,32), kernel_size=3, dilations=(1,2,4)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_c = input_size\n",
    "        for c, d in zip(channels, dilations):\n",
    "            pad = (kernel_size - 1) * d\n",
    "            layers.append(TemporalBlock(in_c, c, kernel_size, 1, d, pad))\n",
    "            in_c = c\n",
    "        self.tcn  = nn.Sequential(*layers)\n",
    "        self.head = nn.Conv1d(in_c, output_size, kernel_size=1)\n",
    "    def forward(self, x):          # [B,L,F]\n",
    "        x = x.transpose(1,2)       # [B,F,L]\n",
    "        h = self.tcn(x)            # [B,C,L]\n",
    "        y = self.head(h)           # [B,2,L]\n",
    "        return y.transpose(1,2)    # [B,L,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c07590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 0.3521 | val 0.1980\n",
      "Epoch 02 | train 0.1600 | val 0.1363\n",
      "Epoch 03 | train 0.1219 | val 0.1138\n",
      "Epoch 04 | train 0.0983 | val 0.0855\n",
      "Epoch 05 | train 0.0824 | val 0.0765\n",
      "Epoch 06 | train 0.0682 | val 0.0669\n",
      "Epoch 07 | train 0.0626 | val 0.0588\n",
      "Epoch 08 | train 0.0557 | val 0.0495\n",
      "Epoch 09 | train 0.0489 | val 0.0460\n",
      "Epoch 10 | train 0.0431 | val 0.0456\n",
      "Epoch 11 | train 0.0414 | val 0.0405\n",
      "Epoch 12 | train 0.0397 | val 0.0412\n",
      "Epoch 13 | train 0.0375 | val 0.0346\n",
      "Epoch 14 | train 0.0384 | val 0.0336\n",
      "Epoch 15 | train 0.0320 | val 0.0470\n",
      "Epoch 16 | train 0.0389 | val 0.0312\n",
      "Epoch 17 | train 0.0311 | val 0.0328\n",
      "Epoch 18 | train 0.0306 | val 0.0329\n",
      "Epoch 19 | train 0.0287 | val 0.0315\n",
      "Epoch 20 | train 0.0313 | val 0.0305\n",
      "Epoch 21 | train 0.0292 | val 0.0297\n",
      "Epoch 22 | train 0.0308 | val 0.0274\n",
      "Epoch 23 | train 0.0255 | val 0.0380\n",
      "Epoch 24 | train 0.0243 | val 0.0302\n",
      "Epoch 25 | train 0.0256 | val 0.0290\n",
      "Epoch 26 | train 0.0262 | val 0.0306\n",
      "Epoch 27 | train 0.0238 | val 0.0302\n",
      "Early stopping.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Torch DataLoaders\n",
    "# -----------------------\n",
    "def to_tensors(Xa, ya): return torch.tensor(Xa, dtype=torch.float32), torch.tensor(ya, dtype=torch.long)\n",
    "Xtr_t, ytr_t = to_tensors(X_train, y_train)\n",
    "Xva_t, yva_t = to_tensors(X_val,   y_val)\n",
    "Xte_t, yte_t = to_tensors(X_test,  y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr_t, ytr_t), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(Xva_t, yva_t), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(Xte_t, yte_t), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -----------------------\n",
    "# Train + Early stopping\n",
    "# -----------------------\n",
    "INPUT_SIZE  = X_train.shape[2]\n",
    "OUTPUT_SIZE = int(np.max([y_train.max(), y_val.max(), y_test.max()])) + 1  # should be 2\n",
    "\n",
    "model = TCN(INPUT_SIZE, OUTPUT_SIZE).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "best_val = float(\"inf\"); patience = 0\n",
    "best_path = os.path.join(MODEL_DIR, \"tcn_model.pt\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(); tr_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)                              # [B,L,2]\n",
    "        loss = criterion(logits.reshape(-1, OUTPUT_SIZE), yb.reshape(-1))\n",
    "        loss.backward(); optimizer.step()\n",
    "        tr_loss += loss.item() * xb.size(0)\n",
    "    tr_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval(); va_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits.reshape(-1, OUTPUT_SIZE), yb.reshape(-1))\n",
    "            va_loss += loss.item() * xb.size(0)\n",
    "    va_loss /= len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1:02d} | train {tr_loss:.4f} | val {va_loss:.4f}\")\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss; patience = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= PATIENCE:\n",
    "            print(\"Early stopping.\"); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "384c1978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'precision': 0.9798201359947357, 'recall': 0.9695062398263701, 'f1_score': 0.974635902470954}\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Evaluate\n",
    "# -----------------------\n",
    "model.load_state_dict(torch.load(best_path)); model.eval()\n",
    "all_preds, all_tgts = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        pred = model(xb).argmax(dim=-1).cpu().numpy()\n",
    "        all_preds.append(pred); all_tgts.append(yb.numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds, axis=0).reshape(-1)\n",
    "all_tgts  = np.concatenate(all_tgts,  axis=0).reshape(-1)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(all_tgts, all_preds, average='binary')\n",
    "metrics = {\"precision\": float(prec), \"recall\": float(rec), \"f1_score\": float(f1)}\n",
    "with open(os.path.join(EVAL_DIR, \"tcn_metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Test metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ff51e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geloq\\AppData\\Local\\Temp\\ipykernel_16024\\4127657880.py:6: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported ONNX: c:\\Users\\geloq\\OneDrive\\Desktop\\pd-keyboard-app\\backend\\ml\\models\\tcn_model.onnx\n",
      "Saved feature config: c:\\Users\\geloq\\OneDrive\\Desktop\\pd-keyboard-app\\backend\\ml\\evaluation\\tcn_feature_config.json\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Export ONNX + feature config\n",
    "# -----------------------\n",
    "dummy = torch.randn(1, SEQ_LEN, INPUT_SIZE, device=DEVICE)\n",
    "onnx_path = os.path.join(MODEL_DIR, \"tcn_model.onnx\")\n",
    "torch.onnx.export(\n",
    "    model, dummy, onnx_path,\n",
    "    input_names=[\"input\"], output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0:\"batch\", 1:\"seq_len\"}, \"output\": {0:\"batch\", 1:\"seq_len\"}},\n",
    "    opset_version=14\n",
    ")\n",
    "print(\"Exported ONNX:\", onnx_path)\n",
    "\n",
    "with open(os.path.join(EVAL_DIR, \"tcn_feature_config.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"features\": FEATS,\n",
    "        \"scaler_mean\": mean.tolist(),\n",
    "        \"scaler_scale\": std.tolist()\n",
    "    }, f, indent=2)\n",
    "print(\"Saved feature config:\", os.path.join(EVAL_DIR, \"tcn_feature_config.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
